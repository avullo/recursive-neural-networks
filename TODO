-- Model

- subclasses hide the particular RNN instantiation
 - test whether model can be templatised and subclasses inherit with particular template parameters
   (look at what I've done during my research when implementing an RNN based search in the space of structured instances)
   - turns out we can have templatised polymorphic implementations (look at tmp.h,cc in root dir)
 
- design patters to build a model
 - in contrast with the above, the factory could directly instantiate a model with particular parameters according to the problem and the desidered output, so there's no need for inheritance

-- Activation functions

- provide ReLU activation and derivative (set it to default for the hidden units?)

-- Recursive NNs

- must be able to deal with a generic form of super-source output/target

- weight initialisation: use suggestions from deep learning specialisation

-- Optimisation algorithm

- implement gradient descent with momentum using exponentially weighted averages
  i.e. use term beta and multiply dW by (1-beta) and set the default to a
  usually found good value (0.9) so as not to tune it and the learning rate as
  well
  
== Long Term == 
- a single, non templatised, implementation able to build and compute with a dynamic set of state transition functions according to the domain

- hidden/output activation functions have their own polymorphic hierarchy with the error minimisation procedure too
===============